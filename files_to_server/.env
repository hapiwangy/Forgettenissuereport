dataset=pubmedqa
# model configs
## BASE_MODEL setting from where to do more finetune > change to other finetune result to do the continue finetuning
BASE_MODEL=meta-llama/Llama-3.2-1B-Instruct
BF16=1

# training configs
MAX_SEQ_LEN=1024
BATCH=8
ACCUM=2
LR=2e-4
EPOCHS=3
SAVE_STEPS=200

# generation configs

## max new token should be changed if output changed
GEN_MAX_NEW_TOKENS=8
## for experiement use 0.0 to reduce the random
GEN_TEMPERATURE=0.0
## choose answer from the probability bigger than below value
## 1.0 means choose from probability, 0.9 means not consider the value at the head and tail 10%
GEN_TOP_P=1.0
## only DO_Sample = True will use the temperature and top_p parameter
GEN_DO_SAMPLE=False

# test config
MODEL_DIR=runs/outputs-trl-llama32-1b-${dataset}
OUT_CSV=result_${dataset}.csv
# testing benchmark = true will used the base model to do the test
TESTING_BENCHMARK=False